<img src="Algorithms.PNG">

### Description:  
#### Machine learning algorithms are programs that uses math and logic to adjust themselves to perform better as they are exposed to more data. The “learning” part of machine learning means that those programs change how they process data over time, much as humans change how they process data by learning.  

# :large_blue_diamond: List of Algorithms :large_blue_diamond:  
:arrow_forward: AdaBoost Classification  
:arrow_forward: AdaBoost Regressor   
:arrow_forward: Anomaly Detection  
:arrow_forward: Apriori Algorithm  
:arrow_forward: Artificial Neural Network   
:arrow_forward: Bagging Classifier   
:arrow_forward: Bayesian Ridge Regression  
:arrow_forward: Bernoulli Restricted Boltzmann Machine  
:arrow_forward: CatBoost Algorithms    
:arrow_forward: Classification and Clustering  
:arrow_forward: Clustering Algorithms  
:arrow_forward: CART (Classification and Regression Trees)     
:arrow_forward: Decision Tree Classification   
:arrow_forward: Decision Tree Regression  
:arrow_forward: Dimensionality Reduction Algorithms  
:arrow_forward: Ensemble Learning Algorithms  
:arrow_forward: Explanatory Algorithms  
:arrow_forward: Gradient Boosting Classification  
:arrow_forward: Generative Adversarial Networks (GANs)    
:arrow_forward: K-Means Clustering Algorithm   
:arrow_forward: K-Nearest Neighbors Algorithm  
:arrow_forward: Logistic Regression    
:arrow_forward: Linear Regression   
:arrow_forward: Nearest Neighbors  
:arrow_forward: NetworkX  
:arrow_forward: Neural Networks Regression  
:arrow_forward: Quantile Regression  
:arrow_forward: Partial Least Squares Regression (PLSR)  
:arrow_forward: Polynomial Regression    
:arrow_forward: Principal Component Classification  
:arrow_forward: Principal Component Regression  
:arrow_forward: Random Forest Classification  
:arrow_forward: Random Forest Regression   
:arrow_forward: RNN Tensorflow  
:arrow_forward: Ridge Regression  
:arrow_forward: Similarity Algorithms  
:arrow_forward: Support Vector Machines (SVM)  
:arrow_forward: Tensorflow  
:arrow_forward: Time Series  
:arrow_forward: XGBoost  

###  AdaBoost
AdaBoost is short for Adaptive Boosting and is a statistical classification meta-algorithm created by Yoav Freund and Robert Schapire in 1995. The meta-estimator begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset. However, the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.      

### Anomaly Detection 
Anomaly detection is identifying data points in data that don't fit the normal patterns.  It is use for identify of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behaviour. Each node or artificial neuron are connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated and send data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.    

### Artificial Neural Network  
Artificial neural networks (ANNs) consist of input, hidden, and output layers with connected neurons (nodes) to simulate the human brain.  

### Bagging classifier  
Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction.  

### Bayesian Ridge Regression  
Bayesian Ridge Regression is similar to linear regression in which the statistical analysis is undertaken within the context of Bayesian inference. It allows a natural mechanism to survive insufficient data or poorly distributed data by formulating linear regression using probability distributors rather than point estimates.  

### Bernoulli Restricted Boltzmann Machine   
Bernoulli Restricted Boltzmann Machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.  

### Decision Tree  
Decision Tree algorithm is a suppervisied machine learning technique and is usded for both classisfication or regression.  Decision Tree use multiple algorithms to decide to split a node into two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. However, the purity of the node increases with respect to the target variable.    

### Gradient Boosting Algorithm    
Gradient Boosting is a machine learning technique used in regression and classifications. Gradient boosting works on building simple or weak prediction models sequentially where each model tries to predict the error left over by the previous model such as overfitting.  

### K-Means Clustering Algorithm  
K-Means clustering is unsupervised machine learning algorithms and is used to solve complex machine learning problems.  

### K-Nearest Neighbors Algorithm  
K-Nearest Neighbors (KNN or k-NN) is used for a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point.  

### Logistic Regression  
Logistic Regression is use for to estimates the probability of an event occurring, such as voted or didn't vote, based on a given dataset of independent variables. Since the outcome is a probability, the dependent variable is bounded between 0 and 1.  

## Gradient Boosting Algorithms:  
### GBM:  
### XGBoost:  
### LightGBM:  
### Catboost:    
CatBoost is an algorithm for gradient boosting on decision trees.  

### Classification and Clustering  
Classification examples are Logistic regression, Naive Bayes classifier, Support vector machines, and other relates to clasification.  However, clustering are k-means clustering algorithm, Fuzzy c-means clustering algorithm, Gaussian (EM) clustering algorithm, and other algoritm relates to clustering.  


## Authors  
### * Tin Hang  
